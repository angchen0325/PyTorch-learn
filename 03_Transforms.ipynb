{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iMjU2cHgiIGhlaWdodD0iMzEwcHgiIHZpZXdCb3g9IjAgMCAyNTYgMzEwIiB2ZXJzaW9uPSIxLjEiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyIgcHJlc2VydmVBc3BlY3RSYXRpbz0ieE1pZFlNaWQiPgogICAgPHRpdGxlPlB5VG9yY2g8L3RpdGxlPgogICAgPGc+CiAgICAgICAgPHBhdGggZD0iTTEyNy44MjUzNzUsMCBMMTI3LjgyNTM3NSw0NS4wNTMyMDYgTDU4LjYwMTQzODUsMTE0LjI3NDU4MSBDMjIuMzcyNDQ5OCwxNTEuMDE5Mjc5IDIyLjM3NjE0NjYsMjEwLjEyNzU5NCA1OC42MTI1MjksMjQ3LjUzODc3MiBMNTkuNzIxNjkxNywyNDguNjY1NzU3IEM5Ni43NDIxNTU1LDI4Ni4zODQ3MiAxNTcuNTExNTk2LDI4Ni4zODQ3MiAxOTUuMjMwNTU5LDI0OC42NjU3NTcgQzIzMi41NzIzMzMsMjEyLjAxNTQ5OCAyMzIuOTQ1NzUxLDE1Mi4wODg4NjcgMTk2LjM1MDgxMywxMTQuMjk1MzI3IEwxOTUuMjMwNTU5LDExMy4xNTY4ODkgTDIxOC4yODEwMzcsOTAuMTA2NDEyIEMyNjguNTcyOTg4LDE0MC4zOTgzNjMgMjY4LjU3Mjk4OCwyMjEuMDc1MDM0IDIxOC4yODEwMzcsMjcxLjcxNjIzNSBDMTY5LjAzNjgzNSwzMjIuMDA4MTg2IDg4LjAxMDkxNDEsMzIyLjAwODE4NiAzNy43MTg5NjMyLDI3MS43MTYyMzUgQy0xMi4wNzAwNjgyLDIyMS45MjcyMDMgLTEyLjU2Nzk1ODUsMTQyLjAxNTgwOCAzNi4yMjUyOTIyLDkxLjYyNDMyOTMgTDM3LjcxODk2MzIsOTAuMTA2NDEyIEwxMjcuODI1Mzc1LDAgWiBNMTczLjIyNzgzMSw1MC45OTA0NTAyIEMxODIuNDg2MzIzLDUwLjk5MDQ1MDIgMTg5Ljk5MTgxNCw1OC40OTU5NDEzIDE4OS45OTE4MTQsNjcuNzU0NDMzOCBDMTg5Ljk5MTgxNCw3Ny4wMTI5MjYzIDE4Mi40ODYzMjMsODQuNTE4NDE3NSAxNzMuMjI3ODMxLDg0LjUxODQxNzUgQzE2My45NjkzMzgsODQuNTE4NDE3NSAxNTYuNDYzODQ3LDc3LjAxMjkyNjMgMTU2LjQ2Mzg0Nyw2Ny43NTQ0MzM4IEMxNTYuNDYzODQ3LDU4LjQ5NTk0MTMgMTYzLjk2OTMzOCw1MC45OTA0NTAyIDE3My4yMjc4MzEsNTAuOTkwNDUwMiBaIiBmaWxsPSIjRUU0QzJDIj48L3BhdGg+CiAgICA8L2c+Cjwvc3ZnPgo=\" style=\"width:6.5%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font size=5 face=\"Helvetica\" color=#EE4B2B><b>\n",
    "Pytorch Tutorial: Transforms\n",
    "</b></font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><font face=\"Helvetica\" size=3><b>Ang Chen</b></font></center>\n",
    "<center><font face=\"Helvetica\" size=3>July, 2024</font></center>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data does not always come in its final processed form that is required for training machine learning algoritms.\n",
    "We use **transforms** to perform some manipulation of the data and make it suitable for training.\n",
    "\n",
    "All TorchVision datasets have two parameters - $\\texttt{transform}$ to modify the features and $\\texttt{target\\_transform}$ to modify the labels - that accept callables containing the transformation logic.\n",
    "The $\\texttt{torchvision.transforms}$ module offers several commonly-used transforms out of the box.\n",
    "\n",
    "The FashionMNIST features are in PIL Image format, and the labels are integers.\n",
    "For training, we nee the features as normalized tensors, and the labels as one-hotencoded tensors.\n",
    "To make these transformations, we use $\\texttt{ToTensor}$ and $\\texttt{Lambda}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.sensor(y), value=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToTensor converts a PIL image or NumPy $\\texttt{ndarray}$ into a $\\texttt{FloatTensor}$, and scales the image's pixel intensity values in the range [0., 1.]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda transforms apply any use-defined ldambda function.\n",
    "Here, we define a function to turn the integer into a one-hot encoded tensor.\n",
    "It first creates a zero tensor of size 10 (the number of labels in our dataset) and calls $\\texttt{scatter\\_}$ which assigns a $\\texttt{value=1}$ on the index as given by the label $\\texttt{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transform = Lambda(\n",
    "    lambda y: torch.zeros(10, dtype=torch.float).scatter_(\n",
    "        dim=0, index=torch.tensor(y), value=1\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
